{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cecc36d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import ollama\n",
    "\n",
    "EMBEDDING_MODEL = 'hf.co/nomic-ai/nomic-embed-text-v2-moe-GGUF'\n",
    "LANGUAGE_MODEL = 'hf.co/tensorblock/scb10x_llama3.1-typhoon2-8b-instruct-GGUF'\n",
    "\n",
    "DATA_DIR = Path('data')\n",
    "\n",
    "# Diccionario que contiene embeddings por materia\n",
    "VECTOR_DB = {} \n",
    "\n",
    "# Memoria independiente por materia\n",
    "CHAT_HISTORY = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2be140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_words(text, max_words=1000, overlap=200):\n",
    "    \"\"\"Divide el texto en chunks con solapamiento.\"\"\"\n",
    "    words = text.split()\n",
    "    step = max(1, max_words - overlap)\n",
    "    for i in range(0, len(words), step):\n",
    "        chunk_words = words[i:i+max_words]\n",
    "        if not chunk_words:\n",
    "            break\n",
    "        yield ' '.join(chunk_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c1f6f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    \"\"\"Carga los datasets y genera embeddings por materia.\"\"\"\n",
    "    for subject_dir in DATA_DIR.iterdir():\n",
    "        if subject_dir.is_dir():\n",
    "            subject = subject_dir.name.lower()\n",
    "            VECTOR_DB[subject] = []\n",
    "            CHAT_HISTORY[subject] = []  # Inicializa historial vacío\n",
    "            files = sorted(subject_dir.glob('*.txt'))\n",
    "\n",
    "            for txt in files:\n",
    "                with txt.open('r', encoding='utf-8') as f:\n",
    "                    text = f.read().strip()\n",
    "                    for chunk in chunk_by_words(text):\n",
    "                        embedding = ollama.embed(\n",
    "                            model=EMBEDDING_MODEL, \n",
    "                            input=chunk\n",
    "                        )['embeddings'][0]\n",
    "                        VECTOR_DB[subject].append((f\"[{txt.name}] {chunk}\", embedding))\n",
    "\n",
    "            print(f\"Materia '{subject}': cargados {len(VECTOR_DB[subject])} chunks desde {len(files)} archivos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e84b399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similaridad_coseno(a, b):\n",
    "    \"\"\"Calcula la similaridad coseno entre dos embeddings.\"\"\"\n",
    "    producto_matriz = sum(x * y for x, y in zip(a, b))\n",
    "    norma_a = sum(x ** 2 for x in a) ** 0.5\n",
    "    norma_b = sum(x ** 2 for x in b) ** 0.5\n",
    "    return producto_matriz / (norma_a * norma_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19314ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_subject(query, subjects):\n",
    "    \"\"\"Clasifica la pregunta y determina la materia más probable.\"\"\"\n",
    "    prompt = f\"\"\"Eres un asistente que clasifica preguntas por materia.\n",
    "Materias disponibles: {', '.join(subjects)}\n",
    "\n",
    "Pregunta: \"{query}\"\n",
    "\n",
    "Responde únicamente con el nombre exacto de la materia más relacionada.\"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=LANGUAGE_MODEL,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': prompt},\n",
    "        ]\n",
    "    )\n",
    "    subject = response['message']['content'].strip().lower()\n",
    "    return subject if subject in subjects else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0a43455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, subject, top_n=8):\n",
    "    \"\"\"Busca los chunks más relevantes dentro de la materia.\"\"\"\n",
    "    query_embedding = ollama.embed(\n",
    "        model=EMBEDDING_MODEL, \n",
    "        input=query\n",
    "    )['embeddings'][0]\n",
    "\n",
    "    similarities = []\n",
    "    for chunk, embedding in VECTOR_DB[subject]:\n",
    "        similarity = similaridad_coseno(query_embedding, embedding)\n",
    "        similarities.append((chunk, similarity))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e2dbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_instruction_prompt(retrieved_knowledge, chat_history):\n",
    "    \"\"\"Crea el prompt final con contexto y memoria.\"\"\"\n",
    "    history_str = \"\\n\".join([f\"Usuario: {u}\\nAsistente: {a}\" for u, a in chat_history])\n",
    "    context_str = \"\\n\".join([f\" - {chunk}\" for chunk, _ in retrieved_knowledge])\n",
    "    return f\"\"\"Eres un chatbot servicial. Usa solo las siguientes piezas de contexto para contestar la pregunta.\n",
    "Si no sabes la respuesta, admite que no lo sabes.\n",
    "No inventes información.\n",
    "\n",
    "Contexto relevante:\n",
    "{context_str}\n",
    "\n",
    "Historial de conversación:\n",
    "{history_str}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc8e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materia 'cplp': cargados 74 chunks desde 3 archivos.\n",
      "Materia 'redes': cargados 3 chunks desde 1 archivos.\n",
      "\n",
      "Respuesta (REDES):\n",
      "Un protocolo de red es el conjunto de reglas que especifican el intercambio de datos u órdenes durante la comunicación entre las entidades que forman parte de una red. Permiten la comunicación y están implementados en las componentes de la red.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Respuesta (REDES):\n",
      "Un protocolo es un conjunto de reglas que especifican cómo deben interactuar los dispositivos dentro de una red, estableciendo cómo se envían y reciben datos entre ellos. Un protocolo puede incluir información sobre la forma en que se codifican los mensajes, el orden en el que se intercambian esos mensajes y las acciones que se deben realizar al recibir un mensaje u otro evento.\n",
      "------------------------------------------------------------\n",
      "Saliendo del chat...\n"
     ]
    }
   ],
   "source": [
    "load_datasets()\n",
    "\n",
    "while True:\n",
    "    input_query = input(\"Haceme una pregunta (o escribe 'salir' para terminar): \").strip()\n",
    "    if input_query.lower() == \"salir\":\n",
    "        print(\"Saliendo del chat...\")\n",
    "        break\n",
    "\n",
    "    # Detectar materia automáticamente\n",
    "    subjects = list(VECTOR_DB.keys())\n",
    "    subject = detect_subject(input_query, subjects)\n",
    "\n",
    "    if not subject:\n",
    "        print(\"⚠️ No pude determinar la materia. Intenta ser más específico.\")\n",
    "        continue\n",
    "\n",
    "    # Recuperar contexto específico de la materia\n",
    "    retrieved_knowledge = retrieve(input_query, subject)\n",
    "\n",
    "    # Construir prompt con memoria de ESA materia\n",
    "    instruction_prompt = build_instruction_prompt(retrieved_knowledge, CHAT_HISTORY[subject])\n",
    "\n",
    "    # Generar respuesta\n",
    "    stream = ollama.chat(\n",
    "        model=LANGUAGE_MODEL,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': instruction_prompt},\n",
    "            {'role': 'user', 'content': input_query},\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nRespuesta ({subject.upper()}):\")\n",
    "    respuesta_completa = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk['message']['content']\n",
    "        respuesta_completa += content\n",
    "        print(content, end='', flush=True)\n",
    "\n",
    "    # Guardar turno en la memoria específica de la materia\n",
    "    CHAT_HISTORY[subject].append((input_query, respuesta_completa))\n",
    "    print(\"\\n\" + \"-\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
